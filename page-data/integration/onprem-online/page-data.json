{"componentChunkName":"component---src-pages-integration-onprem-online-index-mdx","path":"/integration/onprem-online/","result":{"pageContext":{"frontmatter":{"title":"VMware Using IBM Entitled Registry","weight":400},"relativePagePath":"/integration/onprem-online/index.mdx","titleType":"page","MdxNode":{"id":"30edef1b-89ae-5009-8789-8a5af3b64eb1","children":[],"parent":"5c6749d7-830f-5199-91be-ff44b2f094b0","internal":{"content":"---\ntitle: VMware Using IBM Entitled Registry\nweight: 400\n---\n\n- [Introduction](#introduction)\n- [Prepare For Installation](#prepare-for-installation)\n- [Run the Integration Cloud Pak install](#run-the-integration-cloud-pak-install)\n- [Deploy Capabilities](#deploy-capabilities)\n- [Example files](#example-files)\n  - [config.yaml](#configyaml)\n\n## Introduction\n\nThis page describes all the steps on how to deploy the Integration Cloud \nPak to a VMware on premises environment using the IBM Entitled registry. The \nsteps below includes instructions to: \n\n1. Prepare the bastion/installation node for the installation\n2. Run the Integration Cloud Pak installer to deploy to an existing \nOpenShift cluster.\n\n\n## Prepare For Installation\n\nIn many production scenarios the master nodes may not be accessed via ssh, \nwe have to choose bastion node to proceed with the installation. \n\n**Installer Node requirements:**  \n- Sufficient resources of `4cpu 16GB ram ~120 GB Diskspace`\n- OpenShift CLI, which can be installed following the instructions [here on IBM Cloud](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli).\n- Install Docker (v2.2 is compliant and works with Open Shift)\n- Install Kubernetes CLI kubectl \n\nOnce the CLIs are installed, check if you can login to openshift environment:\n\n  1. Get login token from openshift console\n  2. Run the `oc login` command from a terminal shell.\n  3. You should see the cluster logged in message along with list of projects.\n  \n**Installing on Master or Infra node:**  \nThe value of the master, proxy, and management parameters is an array and \ncan have multiple nodes. Due to a limitation from OpenShift, if you want \nto deploy the common services on any OpenShift master or infrastructure node, \nyou must label the node as an OpenShift compute node with the following \ncommand:  \n```\noc label node <master node host name/infrastructure node host name> node-role.kubernetes.io/compute=true\n```\n\n## Run the Integration Cloud Pak install\n\nIntegration Cloud Pak provides a single installer that installs the common \nservices as \nwell loads all the helm charts for integration capabilities. In this \nexample CP4I will be installed on the master node.\n\n1. Download Integration Cloud Pak installer on the installer node. See [Pre-requisites](../pre-reqs) for guidance.\n     \n2. Open a command line window on the boot node, and extract the contents of \nthe Cloud Pak. It is a general recommendation to create a directory in \n/opt and extract into that directory:  \n   ```\n    tar xf ibm-cp-int-2019.4.1-online.tar.gz --directory /opt/cp4i\n   ```\n\n\n  Once untarred, you can navigate to the directory where the package was \nuntarred to and type `tree`.  It will look like the image below:\n\n![](/assets/img/integration/onprem-online/1.untar-cp4i.png)\n\n3. Load the images onto your local docker registry:\n   ```\n   sudo docker load -i installer_files/cluster/images/icp-inception-3.2.2.tgz\n   ```\n\n4. Change to the `installer_files/cluster/` directory. Place the cluster \nconfiguration files (kubeconfig) in the `installer_files/cluster/` directory. If the configuration file is not already present, you can also use the following command after using oc login as admin.  Make sure your file only has one cluster context defined with in it, and that context is the location of your target cluster.\n\n   ```\n   oc config view --minify=true --flatten=true > kubeconfig\n   ```\n\n4. Note down the IP addresses of OpenShift worker nodes. To get the IP \naddresses of the worker nodes, run:\n   ```\n   oc get nodes -o wide\n   ```  \n\n5. Navigate to your cluster directory `/opt/cp4i/installer/cluster`.  \n   \n6. Edit the config.yaml with the information you have collected above. See \nthe example at the end of the page for guidance.\n\n5. Navigate to your cluster directory `/opt/cp4i/installer_files/cluster`.\n6. Edit the config.yaml with the information you have collected above. See the example at the [end of the page](#configyaml) for guidance.  \n\nHere are the fields to update with your respective values based on your environment:\n\n- under cluster_nodes heading -> set the hostnames for `Master`, `Proxy` and `Management`.  For non-prdoduction type system, setting and proxy to the same host is fine.  use the short name for your nodes (e.g. compute1, compute2 etc)\n- under storage_class -> choose your default storage class here - use `oc get sc` to get a list of available storageclasses.\n- docker_user -> `ekey`\n- docker_password -> set this to your entitlement key\n\nInstructions to get your entitlement key can be found [here](https://github.ibm.com/CloudPakOpenContent/cloudpak-entitlement) \n\n9. Run the installer with:\n   ``` \n   sudo docker run -t --net=host -e LICENSE=accept -v $(pwd):/installer/cluster:z -v /var/run:/var/run:z -v /etc/docker:/etc/docker:z --security-opt label:disable ibmcom/icp-inception-amd64:3.2.2 addon -vvv | tee install.log\n   ```\n9. If the namespaces for the different capabilities did not create you can create them manually using the scripts in `installer_files/cluster/resources` e.g. ace.yaml, apic.yaml.  Simply run each script using this syntax\n   ```\n   oc create -f <scriptname>.yaml\n   ```\n\n9. Once the process is complete, you will need to create your `ibm_entitlement_key` secrets in all of the main component namespaces.  You can accomplish this by running the `create_secrets.sh` script. Before doing so, export these two variables\n   ```\n   export DOCKER_REGISTRY_USER=ekey\n   export DOCKER_REGISTRY_PASS=<your entitlement key>\n   ```\n10. It will be helpful to understand what your proxy node address is, as it will be referenced several time when deploying the individual capabilities.  Run this command and take note of its output.\n   ```\n   oc get configmap -n kube-public ibmcloud-cluster-info -o=jsonpath=\"{.data.proxy_address}\"`\n   ```\n\n## Deploy Capabilities\n\nIt is recommended that you install the Tracing capability first\n\n-  [Tracing](../deploy-tracing)\n-  [App Connect](../deploy-integration)\n-  [API Connect](../deploy-api-mgmt)\n-  [MQ](../deploy-queue-manager)\n-  [Event Streams](../deploy-eventstreams)\n-  [Aspera](../deploy-fast-file-transfer)\n-  [DataPower](../deploy-secure-gateway)\n-  [Asset Repository](../deploy-asset-repo)\n\n## Example files\n\nThis section contains examples of files you will be using throughout the installation. Refer to them for guidance on how to populate your own version of the files.\n\n\n### config.yaml\n```\n# Nodes selected to run common services components.\n#\n# The value of the master, proxy, and management parameters is an array,\n# by providing multiple nodes the common services will be configured in\n# a high availability configuration.\n#\n# It is recommended to install the components onto one or more openshift\n# worker nodes. The master, proxy, and management components can all share\n# the same node or set of nodes.\ncluster_nodes:\n  master:\n    - compute1\n  proxy:\n    - compute1\n  management:\n    - compute2\n# This storage class is used to store persistent data for the common services\n# components\nstorage_class: csi-cephfs\n## You can set a different storage class for storing log data.\n## By default it will use the value of storage_class.\n# elasticsearch_storage_class:\n# These settings enable the installer to install common services from the IBM\n# Entitled Registry. You will need to supply your entitlement key as described\n# at https://github.com/ibm/charts\nprivate_registry_enabled: true\nimage_repo: cp.icr.io/cp/icp-foundation\ndocker_username: ekey\ndocker_password: <your_entitlement_key>\ndefault_admin_password: admin\npassword_rules:\n  - '(.*)'\nmanagement_services:\n  # Common services\n  iam-policy-controller: enabled\n  metering: enabled\n  licensing: disabled\n  monitoring: enabled\n  nginx-ingress: enabled\n  common-web-ui: enabled\n  catalog-ui: enabled\n  mcm-kui: enabled\n  logging: enabled\n  audit-logging: disabled\n  system-healthcheck-service: disabled\n  multitenancy-enforcement: disabled\n  configmap-watcher: disabled\n# This section installs the IBM Cloud Pak for Integration Platform Navigator.\n# The navigator will be available after installation at:\n# https://ibm-icp4i-prod-integration.<openshift apps domain>/\narchive_addons:\n  icp4i:\n    namespace: integration\n    repo: local-charts\n    path: icp4icontent/IBM-Cloud-Pak-for-Integration-3.0.0.tgz\n    charts:\n      - name: ibm-icp4i-prod\n        values: {}\n```\n","type":"Mdx","contentDigest":"5dd427a5dbf92426b9fd39ce146868c0","counter":291,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"VMware Using IBM Entitled Registry","weight":400},"exports":{},"rawBody":"---\ntitle: VMware Using IBM Entitled Registry\nweight: 400\n---\n\n- [Introduction](#introduction)\n- [Prepare For Installation](#prepare-for-installation)\n- [Run the Integration Cloud Pak install](#run-the-integration-cloud-pak-install)\n- [Deploy Capabilities](#deploy-capabilities)\n- [Example files](#example-files)\n  - [config.yaml](#configyaml)\n\n## Introduction\n\nThis page describes all the steps on how to deploy the Integration Cloud \nPak to a VMware on premises environment using the IBM Entitled registry. The \nsteps below includes instructions to: \n\n1. Prepare the bastion/installation node for the installation\n2. Run the Integration Cloud Pak installer to deploy to an existing \nOpenShift cluster.\n\n\n## Prepare For Installation\n\nIn many production scenarios the master nodes may not be accessed via ssh, \nwe have to choose bastion node to proceed with the installation. \n\n**Installer Node requirements:**  \n- Sufficient resources of `4cpu 16GB ram ~120 GB Diskspace`\n- OpenShift CLI, which can be installed following the instructions [here on IBM Cloud](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli).\n- Install Docker (v2.2 is compliant and works with Open Shift)\n- Install Kubernetes CLI kubectl \n\nOnce the CLIs are installed, check if you can login to openshift environment:\n\n  1. Get login token from openshift console\n  2. Run the `oc login` command from a terminal shell.\n  3. You should see the cluster logged in message along with list of projects.\n  \n**Installing on Master or Infra node:**  \nThe value of the master, proxy, and management parameters is an array and \ncan have multiple nodes. Due to a limitation from OpenShift, if you want \nto deploy the common services on any OpenShift master or infrastructure node, \nyou must label the node as an OpenShift compute node with the following \ncommand:  \n```\noc label node <master node host name/infrastructure node host name> node-role.kubernetes.io/compute=true\n```\n\n## Run the Integration Cloud Pak install\n\nIntegration Cloud Pak provides a single installer that installs the common \nservices as \nwell loads all the helm charts for integration capabilities. In this \nexample CP4I will be installed on the master node.\n\n1. Download Integration Cloud Pak installer on the installer node. See [Pre-requisites](../pre-reqs) for guidance.\n     \n2. Open a command line window on the boot node, and extract the contents of \nthe Cloud Pak. It is a general recommendation to create a directory in \n/opt and extract into that directory:  \n   ```\n    tar xf ibm-cp-int-2019.4.1-online.tar.gz --directory /opt/cp4i\n   ```\n\n\n  Once untarred, you can navigate to the directory where the package was \nuntarred to and type `tree`.  It will look like the image below:\n\n![](/assets/img/integration/onprem-online/1.untar-cp4i.png)\n\n3. Load the images onto your local docker registry:\n   ```\n   sudo docker load -i installer_files/cluster/images/icp-inception-3.2.2.tgz\n   ```\n\n4. Change to the `installer_files/cluster/` directory. Place the cluster \nconfiguration files (kubeconfig) in the `installer_files/cluster/` directory. If the configuration file is not already present, you can also use the following command after using oc login as admin.  Make sure your file only has one cluster context defined with in it, and that context is the location of your target cluster.\n\n   ```\n   oc config view --minify=true --flatten=true > kubeconfig\n   ```\n\n4. Note down the IP addresses of OpenShift worker nodes. To get the IP \naddresses of the worker nodes, run:\n   ```\n   oc get nodes -o wide\n   ```  \n\n5. Navigate to your cluster directory `/opt/cp4i/installer/cluster`.  \n   \n6. Edit the config.yaml with the information you have collected above. See \nthe example at the end of the page for guidance.\n\n5. Navigate to your cluster directory `/opt/cp4i/installer_files/cluster`.\n6. Edit the config.yaml with the information you have collected above. See the example at the [end of the page](#configyaml) for guidance.  \n\nHere are the fields to update with your respective values based on your environment:\n\n- under cluster_nodes heading -> set the hostnames for `Master`, `Proxy` and `Management`.  For non-prdoduction type system, setting and proxy to the same host is fine.  use the short name for your nodes (e.g. compute1, compute2 etc)\n- under storage_class -> choose your default storage class here - use `oc get sc` to get a list of available storageclasses.\n- docker_user -> `ekey`\n- docker_password -> set this to your entitlement key\n\nInstructions to get your entitlement key can be found [here](https://github.ibm.com/CloudPakOpenContent/cloudpak-entitlement) \n\n9. Run the installer with:\n   ``` \n   sudo docker run -t --net=host -e LICENSE=accept -v $(pwd):/installer/cluster:z -v /var/run:/var/run:z -v /etc/docker:/etc/docker:z --security-opt label:disable ibmcom/icp-inception-amd64:3.2.2 addon -vvv | tee install.log\n   ```\n9. If the namespaces for the different capabilities did not create you can create them manually using the scripts in `installer_files/cluster/resources` e.g. ace.yaml, apic.yaml.  Simply run each script using this syntax\n   ```\n   oc create -f <scriptname>.yaml\n   ```\n\n9. Once the process is complete, you will need to create your `ibm_entitlement_key` secrets in all of the main component namespaces.  You can accomplish this by running the `create_secrets.sh` script. Before doing so, export these two variables\n   ```\n   export DOCKER_REGISTRY_USER=ekey\n   export DOCKER_REGISTRY_PASS=<your entitlement key>\n   ```\n10. It will be helpful to understand what your proxy node address is, as it will be referenced several time when deploying the individual capabilities.  Run this command and take note of its output.\n   ```\n   oc get configmap -n kube-public ibmcloud-cluster-info -o=jsonpath=\"{.data.proxy_address}\"`\n   ```\n\n## Deploy Capabilities\n\nIt is recommended that you install the Tracing capability first\n\n-  [Tracing](../deploy-tracing)\n-  [App Connect](../deploy-integration)\n-  [API Connect](../deploy-api-mgmt)\n-  [MQ](../deploy-queue-manager)\n-  [Event Streams](../deploy-eventstreams)\n-  [Aspera](../deploy-fast-file-transfer)\n-  [DataPower](../deploy-secure-gateway)\n-  [Asset Repository](../deploy-asset-repo)\n\n## Example files\n\nThis section contains examples of files you will be using throughout the installation. Refer to them for guidance on how to populate your own version of the files.\n\n\n### config.yaml\n```\n# Nodes selected to run common services components.\n#\n# The value of the master, proxy, and management parameters is an array,\n# by providing multiple nodes the common services will be configured in\n# a high availability configuration.\n#\n# It is recommended to install the components onto one or more openshift\n# worker nodes. The master, proxy, and management components can all share\n# the same node or set of nodes.\ncluster_nodes:\n  master:\n    - compute1\n  proxy:\n    - compute1\n  management:\n    - compute2\n# This storage class is used to store persistent data for the common services\n# components\nstorage_class: csi-cephfs\n## You can set a different storage class for storing log data.\n## By default it will use the value of storage_class.\n# elasticsearch_storage_class:\n# These settings enable the installer to install common services from the IBM\n# Entitled Registry. You will need to supply your entitlement key as described\n# at https://github.com/ibm/charts\nprivate_registry_enabled: true\nimage_repo: cp.icr.io/cp/icp-foundation\ndocker_username: ekey\ndocker_password: <your_entitlement_key>\ndefault_admin_password: admin\npassword_rules:\n  - '(.*)'\nmanagement_services:\n  # Common services\n  iam-policy-controller: enabled\n  metering: enabled\n  licensing: disabled\n  monitoring: enabled\n  nginx-ingress: enabled\n  common-web-ui: enabled\n  catalog-ui: enabled\n  mcm-kui: enabled\n  logging: enabled\n  audit-logging: disabled\n  system-healthcheck-service: disabled\n  multitenancy-enforcement: disabled\n  configmap-watcher: disabled\n# This section installs the IBM Cloud Pak for Integration Platform Navigator.\n# The navigator will be available after installation at:\n# https://ibm-icp4i-prod-integration.<openshift apps domain>/\narchive_addons:\n  icp4i:\n    namespace: integration\n    repo: local-charts\n    path: icp4icontent/IBM-Cloud-Pak-for-Integration-3.0.0.tgz\n    charts:\n      - name: ibm-icp4i-prod\n        values: {}\n```\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/integration/onprem-online/index.mdx"}}}}