{"componentChunkName":"component---src-pages-integration-onprem-offline-index-mdx","path":"/integration/onprem-offline/","result":{"pageContext":{"frontmatter":{"title":"VMware using Offline Images","weight":400},"relativePagePath":"/integration/onprem-offline/index.mdx","titleType":"page","MdxNode":{"id":"df44a2b3-9e4c-5e1d-a222-7ac11258d191","children":[],"parent":"6cc8ac23-6c6a-5d5a-848c-416c62cf0f8d","internal":{"content":"---\ntitle: VMware using Offline Images\nweight: 400\n---\n\n- [Setting the max_map_count](#setting-the-max_map_count)\n- [Download and extract the image](#download-and-extract-the-image)\n- [Creating config.yaml](#creating-configyaml)\n- [Creating getAllRec.sh](#creating-getallrecsh)\n- [Starting the install process](#starting-the-install-process)\n- [Creating the correct kubeconfig](#creating-the-correct-kubeconfig)\n- [Uninstalling Common Services](#uninstalling-common-services)\n\n**NOTE: Make sure you have 200GB or more on your installer node. If you have less, then you can download the offline image, extract and delete the original file**\n\n## Setting the max_map_count\n\nSSH into all your worker and storage nodes and set the max_map_count to 262144.\n\n```bash\nsudo sysctl -w vm.max_map_count=262144\necho \"vm.max_map_count=262144\" | sudo tee -a /etc/sysctl.conf\n```\n\n## Download and extract the image\n\nSSH into your installer node. Go to `/opt` dir and download the image there. Look at [Pre-requisites](../pre-reqs) to learn how to get the offline image.\n\n```bash\ncd /opt\nmkdir cp4ioffline\ntar xf ibm-cp-int-2019.4.1-offline.tar.gz --directory /opt/cp4ioffline\n```\n\nNow you can extract the images and load them into docker.\n\n```bash\ncd cp4ioffline\ntar xvf installer_files/cluster/images/common-services-armonk-x86_64.tar.gz -O | sudo docker load\n```\n\nYou can delete `ibm-cp-int-2019.4.1-offline.tar.gz` now if you're low on space.\n\n## Creating config.yaml\n\nNow you have to configure your `cp4ioffline/installer_files/cluster/config.yaml`. It is always a good idea to create a backup of the default `cluster.yaml`.\n\n - You can use your OpenShift master and infrastructure nodes here, or install these components to dedicated OpenShift compute nodes. You can specify more than one node for each type to build a high availability cluster. Use the command `oc get nodes` to obtain these values.\n  ```yaml\n  cluster_nodes:\n  master:\n    - your-openshift-dedicated-node-to-deploy-icp-master-components\n  proxy:\n    - your-openshift-dedicated-node-to-deploy-icp-proxy-components>\n  management:\n    - your-openshift-dedicated-node-to-deploy-icp-management-components>\n  ```\n\n  - Set the default_admin_password. The password must meet the default password enforcement rule '^([a-zA-Z0-9\\-]{32,})$' . Optionally, you can define one or more rules as regular expressions in an array list that the password must pass. The rules are written as regular expressions that are supported by the Go programming language. To define a set of password rules, add the following parameter and values to the file:\n\n  ```yaml\n    password_rules:\n    - '^.{10,}'\n    - '.*[!@#\\$%\\^&\\*].*'\n  ``` \n  To disable the password_rule, add (.*).\n\n  ```yaml\n    password_rules:\n    - '(.*)'\n  ```\n\n  - Finally, you'll have define what capabilities you would like to install under `archive_addons:`. By default it installs all the capabilities. But depending on your requirements, you can pick and choose. For example defined below is a cluster with `mq` and `tracing`\n  **Note: You must deploy `icp4i` also referred to as common services, otherwise you can't deploy any other capabilities**\n\n  ```yaml\n    archive_addons:\n      icp4i:\n        namespace: integration\n        repo: local-charts\n        path: icp4icontent/IBM-Cloud-Pak-for-Integration-3.0.0.tgz\n        charts:\n          - name: ibm-icp4i-prod\n            values: {}\n      mq:\n        namespace: mq\n        repo: local-charts\n        path: icp4icontent/IBM-MQ-Advanced-for-IBM-Cloud-Pak-for-Integration-5.0.0.tgz\n      tracing:\n        namespace: tracing\n        repo: local-charts\n        path: icp4icontent/IBM-Cloud-Pak-for-Integration-Operations-Dashboard-1.0.1.tgz\n      \n   \n  ```\n\n  Additionally, here is an example that deploys everything [`config.yaml`](/assets/integration/utils/config.yaml)\n\n## Creating getAllRec.sh\n\nWhen the installer fails, this script will echo all the pods that are up and running and pods that are failing.\n\n```bash\ncd /opt\ntouch getAllRec.sh\nsudo chmod 755 getAllRec.sh\n```\n\nYou can get the [`getAllRec.sh`](/assets/integration/utils/getAllRec.sh) file from here.\n\nTo run `getAllRec.sh` you need to pass in a namespace. Usually on a failed install, the `kube-system` namespace gives us the most information regarding the failure.\n\n```bash\n./getAllRec.sh kube-system\n```\n\n## Creating the correct kubeconfig\n\nFirst you have to copy our kubeconfig to the cluster directory. \n\n```bash\ncd /opt/cp4ioffline/installer_files/cluster\noc config view --minify=true --flatten=true > kubeconfig\n```\n\nIt should look similar to this. **Note:** Make sure it has `insecure-skip-tls-verify: true`config.\n\n```yaml\napiVersion: v1\nclusters:\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://api.mislam.ocp.csplab.local:6443\n  name: api-mislam-ocp-csplab-local:6443\ncontexts:\n- context:\n    cluster: api-mislam-ocp-csplab-local:6443\n    namespace: openshift-image-registry\n    user: kube:admin/api-mislam-ocp-csplab-local:6443\n  name: openshift-image-registry/api-mislam-ocp-csplab-local:6443/kube:admin\ncurrent-context: openshift-image-registry/api-mislam-ocp-csplab-local:6443/kube:admin\nkind: Config\npreferences: {}\nusers:\n- name: kube:admin/api-mislam-ocp-csplab-local:6443\n  user:\n    token: 3lg2U7vUcu-ovvsrK-sYDSWg3t5vLcBPY83DkvL44Is\n```\n\n**Alternatively** you can copy it from our Openshift Cluster Auth directory to here. \n\n```bash\ncp /opt/myocpcluster/auth/kubeconfig /opt/cp4ioffline/installer_files/cluster\n```\n\n## Checking Docker Login\n\nRun this command to see if you can login to docker \n\n```bash\nsudo docker login $(oc registry info) -u kubeadmin -p $(oc whoami -t)\n```\n\n## Starting the install process\n\nNow you run the installer. Notice it's a nohup job (runs on the background) and the logs are written to `install.log` so you can close your terminal and leave but the installer will keep on going on the server. And log back in and look at `install.log` to see progress.\n\n```bash\nnohup sudo docker run -t --net=host -e LICENSE=accept -v $(pwd):/installer/cluster:z -v /var/run:/var/run:z -v /etc/docker:/etc/docker:z --security-opt label:disable ibmcom/icp-inception-amd64:3.2.2 addon -vvv | tee install.log &\n```\n\n**NOTE: If the installer fails, run the getAllRec.sh script to check if common services is up or not. If it isn't up, you can run the installer again. If it is up but one of the capabilities failed tyo get pushed, then that capability can be applied individually** \n## Uninstalling Common Services\n\nIn case the installer fails on the same step multiple times, it's better to uninstall and try again. To uninstall\n\n```bash\nnohup sudo docker run -t --net=host -e LICENSE=accept -v $(pwd):/installer/cluster:z -v /var/run:/var/run:z -v /etc/docker:/etc/docker:z --security-opt label:disable ibmcom/icp-inception-amd64:3.2.2 uninstall-with-openshift | tee install.log &\n```\n\n\n","type":"Mdx","contentDigest":"ccbdc9b5fcd88994dcb0091e052b3818","counter":261,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"VMware using Offline Images","weight":400},"exports":{},"rawBody":"---\ntitle: VMware using Offline Images\nweight: 400\n---\n\n- [Setting the max_map_count](#setting-the-max_map_count)\n- [Download and extract the image](#download-and-extract-the-image)\n- [Creating config.yaml](#creating-configyaml)\n- [Creating getAllRec.sh](#creating-getallrecsh)\n- [Starting the install process](#starting-the-install-process)\n- [Creating the correct kubeconfig](#creating-the-correct-kubeconfig)\n- [Uninstalling Common Services](#uninstalling-common-services)\n\n**NOTE: Make sure you have 200GB or more on your installer node. If you have less, then you can download the offline image, extract and delete the original file**\n\n## Setting the max_map_count\n\nSSH into all your worker and storage nodes and set the max_map_count to 262144.\n\n```bash\nsudo sysctl -w vm.max_map_count=262144\necho \"vm.max_map_count=262144\" | sudo tee -a /etc/sysctl.conf\n```\n\n## Download and extract the image\n\nSSH into your installer node. Go to `/opt` dir and download the image there. Look at [Pre-requisites](../pre-reqs) to learn how to get the offline image.\n\n```bash\ncd /opt\nmkdir cp4ioffline\ntar xf ibm-cp-int-2019.4.1-offline.tar.gz --directory /opt/cp4ioffline\n```\n\nNow you can extract the images and load them into docker.\n\n```bash\ncd cp4ioffline\ntar xvf installer_files/cluster/images/common-services-armonk-x86_64.tar.gz -O | sudo docker load\n```\n\nYou can delete `ibm-cp-int-2019.4.1-offline.tar.gz` now if you're low on space.\n\n## Creating config.yaml\n\nNow you have to configure your `cp4ioffline/installer_files/cluster/config.yaml`. It is always a good idea to create a backup of the default `cluster.yaml`.\n\n - You can use your OpenShift master and infrastructure nodes here, or install these components to dedicated OpenShift compute nodes. You can specify more than one node for each type to build a high availability cluster. Use the command `oc get nodes` to obtain these values.\n  ```yaml\n  cluster_nodes:\n  master:\n    - your-openshift-dedicated-node-to-deploy-icp-master-components\n  proxy:\n    - your-openshift-dedicated-node-to-deploy-icp-proxy-components>\n  management:\n    - your-openshift-dedicated-node-to-deploy-icp-management-components>\n  ```\n\n  - Set the default_admin_password. The password must meet the default password enforcement rule '^([a-zA-Z0-9\\-]{32,})$' . Optionally, you can define one or more rules as regular expressions in an array list that the password must pass. The rules are written as regular expressions that are supported by the Go programming language. To define a set of password rules, add the following parameter and values to the file:\n\n  ```yaml\n    password_rules:\n    - '^.{10,}'\n    - '.*[!@#\\$%\\^&\\*].*'\n  ``` \n  To disable the password_rule, add (.*).\n\n  ```yaml\n    password_rules:\n    - '(.*)'\n  ```\n\n  - Finally, you'll have define what capabilities you would like to install under `archive_addons:`. By default it installs all the capabilities. But depending on your requirements, you can pick and choose. For example defined below is a cluster with `mq` and `tracing`\n  **Note: You must deploy `icp4i` also referred to as common services, otherwise you can't deploy any other capabilities**\n\n  ```yaml\n    archive_addons:\n      icp4i:\n        namespace: integration\n        repo: local-charts\n        path: icp4icontent/IBM-Cloud-Pak-for-Integration-3.0.0.tgz\n        charts:\n          - name: ibm-icp4i-prod\n            values: {}\n      mq:\n        namespace: mq\n        repo: local-charts\n        path: icp4icontent/IBM-MQ-Advanced-for-IBM-Cloud-Pak-for-Integration-5.0.0.tgz\n      tracing:\n        namespace: tracing\n        repo: local-charts\n        path: icp4icontent/IBM-Cloud-Pak-for-Integration-Operations-Dashboard-1.0.1.tgz\n      \n   \n  ```\n\n  Additionally, here is an example that deploys everything [`config.yaml`](/assets/integration/utils/config.yaml)\n\n## Creating getAllRec.sh\n\nWhen the installer fails, this script will echo all the pods that are up and running and pods that are failing.\n\n```bash\ncd /opt\ntouch getAllRec.sh\nsudo chmod 755 getAllRec.sh\n```\n\nYou can get the [`getAllRec.sh`](/assets/integration/utils/getAllRec.sh) file from here.\n\nTo run `getAllRec.sh` you need to pass in a namespace. Usually on a failed install, the `kube-system` namespace gives us the most information regarding the failure.\n\n```bash\n./getAllRec.sh kube-system\n```\n\n## Creating the correct kubeconfig\n\nFirst you have to copy our kubeconfig to the cluster directory. \n\n```bash\ncd /opt/cp4ioffline/installer_files/cluster\noc config view --minify=true --flatten=true > kubeconfig\n```\n\nIt should look similar to this. **Note:** Make sure it has `insecure-skip-tls-verify: true`config.\n\n```yaml\napiVersion: v1\nclusters:\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://api.mislam.ocp.csplab.local:6443\n  name: api-mislam-ocp-csplab-local:6443\ncontexts:\n- context:\n    cluster: api-mislam-ocp-csplab-local:6443\n    namespace: openshift-image-registry\n    user: kube:admin/api-mislam-ocp-csplab-local:6443\n  name: openshift-image-registry/api-mislam-ocp-csplab-local:6443/kube:admin\ncurrent-context: openshift-image-registry/api-mislam-ocp-csplab-local:6443/kube:admin\nkind: Config\npreferences: {}\nusers:\n- name: kube:admin/api-mislam-ocp-csplab-local:6443\n  user:\n    token: 3lg2U7vUcu-ovvsrK-sYDSWg3t5vLcBPY83DkvL44Is\n```\n\n**Alternatively** you can copy it from our Openshift Cluster Auth directory to here. \n\n```bash\ncp /opt/myocpcluster/auth/kubeconfig /opt/cp4ioffline/installer_files/cluster\n```\n\n## Checking Docker Login\n\nRun this command to see if you can login to docker \n\n```bash\nsudo docker login $(oc registry info) -u kubeadmin -p $(oc whoami -t)\n```\n\n## Starting the install process\n\nNow you run the installer. Notice it's a nohup job (runs on the background) and the logs are written to `install.log` so you can close your terminal and leave but the installer will keep on going on the server. And log back in and look at `install.log` to see progress.\n\n```bash\nnohup sudo docker run -t --net=host -e LICENSE=accept -v $(pwd):/installer/cluster:z -v /var/run:/var/run:z -v /etc/docker:/etc/docker:z --security-opt label:disable ibmcom/icp-inception-amd64:3.2.2 addon -vvv | tee install.log &\n```\n\n**NOTE: If the installer fails, run the getAllRec.sh script to check if common services is up or not. If it isn't up, you can run the installer again. If it is up but one of the capabilities failed tyo get pushed, then that capability can be applied individually** \n## Uninstalling Common Services\n\nIn case the installer fails on the same step multiple times, it's better to uninstall and try again. To uninstall\n\n```bash\nnohup sudo docker run -t --net=host -e LICENSE=accept -v $(pwd):/installer/cluster:z -v /var/run:/var/run:z -v /etc/docker:/etc/docker:z --security-opt label:disable ibmcom/icp-inception-amd64:3.2.2 uninstall-with-openshift | tee install.log &\n```\n\n\n","fileAbsolutePath":"/home/travis/build/ibm-cloud-architecture/cloudpak8s/src/pages/integration/onprem-offline/index.mdx"}}}}